{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import imageio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['anger','contempt','disgust','fear','happy','sadness','surprise']\n",
    "data_path = r'./data/CK+48'\n",
    "paths = []\n",
    "img_labels = []\n",
    "\n",
    "for idx,cate in enumerate(categories):\n",
    "    cate_path = data_path + '/' + cate\n",
    "    img_list = os.listdir(cate_path)\n",
    "    for img in img_list:\n",
    "        img_path = cate_path + '/' + img\n",
    "        paths.append(img_path)\n",
    "        img_labels.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    # Characterizes a dataset for PyTorch\n",
    "    def __init__(self, paths, img_labels, transform):\n",
    "        'Initialization'\n",
    "        self.labels = img_labels\n",
    "        self.paths = paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Generates one sample of data\n",
    "        # Select sample\n",
    "        path = self.paths[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        raw = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        X = transform(raw) \n",
    "        y = self.labels[index]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "dataset = Dataset(paths, img_labels, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "valid_size = len(dataset) - train_size\n",
    "train_set, valid_set = torch.utils.data.random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 4 # (4, 4) kernel\n",
    "init_channels = 8 # initial number of filters\n",
    "image_channels = 1 # CK+ images are grayscale\n",
    "latent_dim = 16 # latent dimension for sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a Conv VAE\n",
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvVAE, self).__init__()\n",
    " \n",
    "        # encoder\n",
    "        self.enc1 = nn.Conv2d(\n",
    "            in_channels=image_channels, out_channels=init_channels, kernel_size=kernel_size, \n",
    "            stride=2, padding=1\n",
    "        )\n",
    "        self.enc2 = nn.Conv2d(\n",
    "            in_channels=init_channels, out_channels=init_channels*2, kernel_size=kernel_size, \n",
    "            stride=2, padding=1\n",
    "        )\n",
    "        self.enc3 = nn.Conv2d(\n",
    "            in_channels=init_channels*2, out_channels=init_channels*4, kernel_size=kernel_size, \n",
    "            stride=2, padding=1\n",
    "        )\n",
    "        self.enc4 = nn.Conv2d(\n",
    "            in_channels=init_channels*4, out_channels=64, kernel_size=kernel_size, \n",
    "            stride=2, padding=0\n",
    "        )\n",
    "        # fully connected layers for learning representations\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(128, latent_dim)\n",
    "        self.fc2 = nn.Linear(latent_dim, 64)\n",
    "        \n",
    "        # decoder \n",
    "        self.dec1 = nn.ConvTranspose2d(\n",
    "            in_channels=64, out_channels=init_channels*8, kernel_size=kernel_size, \n",
    "            stride=2, padding=0\n",
    "        )\n",
    "        self.dec2 = nn.ConvTranspose2d(\n",
    "            in_channels=init_channels*8, out_channels=init_channels*4, kernel_size=kernel_size, \n",
    "            stride=2, padding=1\n",
    "        )\n",
    "        self.dec3 = nn.ConvTranspose2d(\n",
    "            in_channels=init_channels*4, out_channels=init_channels*2, kernel_size=kernel_size, \n",
    "            stride=2, padding=1\n",
    "        )\n",
    "        self.dec4 = nn.ConvTranspose2d(\n",
    "            in_channels=init_channels*2, out_channels=image_channels, kernel_size=kernel_size, \n",
    "            stride=2, padding=1\n",
    "        )\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling\n",
    "        return sample\n",
    " \n",
    "    def forward(self, x):\n",
    "        # encoding\n",
    "        x = F.relu(self.enc1(x)) \n",
    "        #print('1: x = {}'.format(x.shape)) # shape = (N,8,24,24)\n",
    "        x = F.relu(self.enc2(x))\n",
    "        #print('2: x = {}'.format(x.shape)) # shape = (N,16,12,12)\n",
    "        x = F.relu(self.enc3(x))\n",
    "        #print('3: x = {}'.format(x.shape)) # shape = (N,32,6,6)\n",
    "        x = F.relu(self.enc4(x))\n",
    "        #print('4: x = {}'.format(x.shape)) # shape = (N,64,2,2)\n",
    "        \n",
    "        batch, _, _, _ = x.shape\n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(batch, -1)\n",
    "        #print('5: x = {}'.format(x.shape)) # shape = (N,64)\n",
    "        hidden = self.fc1(x)\n",
    "        #print('6: x = {}'.format(x.shape)) # shape = (N,64)\n",
    "        \n",
    "        # get `mu` and `log_var`\n",
    "        mu = self.fc_mu(hidden) # shape = (N,16)\n",
    "        log_var = self.fc_log_var(hidden) # shape = (N.16)\n",
    "        \n",
    "        # get the latent vector through reparameterization\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        #print('7: z = {}'.format(z.shape)) # shape = (N,16)\n",
    "        z = self.fc2(z)\n",
    "        #print('8: z = {}'.format(z.shape)) # shape = (N,64)\n",
    "        z = z.view(-1, 64, 1, 1)\n",
    "        #print('9: z = {}'.format(z.shape)) # shape = (N,64,1,1)\n",
    " \n",
    "        # decoding\n",
    "        x = F.adaptive_avg_pool2d(z,2)\n",
    "        #print('9.5: z = {}'.format(x.shape)) # shape = (N,64,2,2)\n",
    "        x = F.relu(self.dec1(x))\n",
    "        #print('10: x = {}'.format(x.shape)) # shape = (N,64,6,6)\n",
    "        x = F.relu(self.dec2(x))\n",
    "        #print('11: x = {}'.format(x.shape)) # shape = (N,32,12,12)\n",
    "        x = F.relu(self.dec3(x))\n",
    "        #print('12: x = {}'.format(x.shape)) # shape = (N,16,24,24)\n",
    "        reconstruction = torch.sigmoid(self.dec4(x))\n",
    "        #print('13: reconstruction = {}'.format(reconstruction.shape)) # shape = (N,1,48,48)\n",
    "        return reconstruction, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_loss(bce_loss, mu, logvar):\n",
    "    \"\"\"\n",
    "    This function will add the reconstruction loss (BCELoss) and the \n",
    "    KL-Divergence.\n",
    "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    :param bce_loss: recontruction loss\n",
    "    :param mu: the mean from the latent vector\n",
    "    :param logvar: log variance from the latent vector\n",
    "    \"\"\"\n",
    "    BCE = bce_loss \n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# initialize the model\n",
    "model = ConvVAE().to(device)\n",
    "# set the learning parameters\n",
    "lr = 0.001\n",
    "epochs = 400\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss(reduction='sum')\n",
    "# a list to save all the reconstructed images in PyTorch grid format\n",
    "grid_images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, dataset, device, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "    \n",
    "    for i, data in enumerate(dataloader):\n",
    "        counter += 1\n",
    "        inputs, labels = data[0].to(device), data[1]\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        reconstruction, mu, logvar = model(inputs)\n",
    "        bce_loss = criterion(reconstruction, inputs)\n",
    "        loss = final_loss(bce_loss, mu, logvar)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    train_loss = running_loss / counter \n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, dataset, device, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader):\n",
    "            counter += 1\n",
    "            inputs, labels = data[0].to(device), data[1]\n",
    "            \n",
    "            reconstruction, mu, logvar = model(inputs)\n",
    "            bce_loss = criterion(reconstruction, inputs)\n",
    "            loss = final_loss(bce_loss, mu, logvar)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "            # save the last batch input and output of every epoch\n",
    "            if i == int(len(dataset)/dataloader.batch_size) - 1:\n",
    "                recon_images = reconstruction\n",
    "    val_loss = running_loss / counter\n",
    "    return val_loss, recon_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_reconstructed_images(recon_images, epoch):\n",
    "    SAVE_DIR = r\"C:\\Users\\trongduong\\[QML Study]\\VAEoutputs\"\n",
    "    local_path = \"output\" + str(epoch) + \".jpg\"\n",
    "    path = os.path.join(SAVE_DIR, local_path)\n",
    "    save_image(recon_images.cpu(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_vid(images):\n",
    "    to_pil_image = transforms.ToPILImage()\n",
    "    imgs = [np.array(to_pil_image(img)) for img in images]\n",
    "    #print(len(imgs), imgs[0].shape)\n",
    "    SAVE_DIR = r\"C:\\Users\\trongduong\\[QML Study]\\VAEoutputs\"\n",
    "    local_path = \"generated_images.gif\"\n",
    "    path = os.path.join(SAVE_DIR, local_path)\n",
    "    imageio.mimsave(path, imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_loss_plot(train_loss, valid_loss):\n",
    "    # loss plots\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(train_loss, color='orange', label='train loss')\n",
    "    plt.plot(valid_loss, color='red', label='validataion loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    SAVE_DIR = r\"C:\\Users\\trongduong\\[QML Study]\\VAEoutputs\"\n",
    "    local_path = \"loss.jpg\"\n",
    "    path = os.path.join(SAVE_DIR, local_path)\n",
    "    \n",
    "    plt.savefig(path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 400\n",
      "Train Loss: 49403.4516\n",
      "Val Loss: 43035.0192\n",
      "Epoch 2 of 400\n",
      "Train Loss: 45946.9148\n",
      "Val Loss: 39877.7086\n",
      "Epoch 3 of 400\n",
      "Train Loss: 43238.5928\n",
      "Val Loss: 38406.2048\n",
      "Epoch 4 of 400\n",
      "Train Loss: 42124.5255\n",
      "Val Loss: 37714.5201\n",
      "Epoch 5 of 400\n",
      "Train Loss: 41736.5023\n",
      "Val Loss: 37641.8448\n",
      "Epoch 6 of 400\n",
      "Train Loss: 41467.6645\n",
      "Val Loss: 37394.9492\n",
      "Epoch 7 of 400\n",
      "Train Loss: 41284.1114\n",
      "Val Loss: 37266.3685\n",
      "Epoch 8 of 400\n",
      "Train Loss: 41213.5941\n",
      "Val Loss: 37341.6576\n",
      "Epoch 9 of 400\n",
      "Train Loss: 41196.7098\n",
      "Val Loss: 37214.8152\n",
      "Epoch 10 of 400\n",
      "Train Loss: 41030.5764\n",
      "Val Loss: 37002.8908\n",
      "Epoch 11 of 400\n",
      "Train Loss: 40824.3405\n",
      "Val Loss: 36791.1675\n",
      "Epoch 12 of 400\n",
      "Train Loss: 40596.2101\n",
      "Val Loss: 36747.7361\n",
      "Epoch 13 of 400\n",
      "Train Loss: 40441.6181\n",
      "Val Loss: 36512.9326\n",
      "Epoch 14 of 400\n",
      "Train Loss: 40281.4402\n",
      "Val Loss: 36417.4099\n",
      "Epoch 15 of 400\n",
      "Train Loss: 40110.0515\n",
      "Val Loss: 36378.2609\n",
      "Epoch 16 of 400\n",
      "Train Loss: 40004.3235\n",
      "Val Loss: 36181.5945\n",
      "Epoch 17 of 400\n",
      "Train Loss: 39916.2180\n",
      "Val Loss: 36119.5073\n",
      "Epoch 18 of 400\n",
      "Train Loss: 39853.7744\n",
      "Val Loss: 36227.7571\n",
      "Epoch 19 of 400\n",
      "Train Loss: 39892.2610\n",
      "Val Loss: 36151.2122\n",
      "Epoch 20 of 400\n",
      "Train Loss: 39793.9406\n",
      "Val Loss: 36007.2702\n",
      "Epoch 21 of 400\n",
      "Train Loss: 39742.0354\n",
      "Val Loss: 36138.2956\n",
      "Epoch 22 of 400\n",
      "Train Loss: 39771.3682\n",
      "Val Loss: 35981.9630\n",
      "Epoch 23 of 400\n",
      "Train Loss: 39700.1133\n",
      "Val Loss: 35955.2502\n",
      "Epoch 24 of 400\n",
      "Train Loss: 39655.4806\n",
      "Val Loss: 35894.8913\n",
      "Epoch 25 of 400\n",
      "Train Loss: 39646.2224\n",
      "Val Loss: 36172.0887\n",
      "Epoch 26 of 400\n",
      "Train Loss: 39650.7641\n",
      "Val Loss: 35866.7723\n",
      "Epoch 27 of 400\n",
      "Train Loss: 39598.5667\n",
      "Val Loss: 35875.7221\n",
      "Epoch 28 of 400\n",
      "Train Loss: 39580.7981\n",
      "Val Loss: 35820.9286\n",
      "Epoch 29 of 400\n",
      "Train Loss: 39553.6067\n",
      "Val Loss: 35888.7513\n",
      "Epoch 30 of 400\n",
      "Train Loss: 39516.8479\n",
      "Val Loss: 35863.2759\n",
      "Epoch 31 of 400\n",
      "Train Loss: 39508.2230\n",
      "Val Loss: 35779.1258\n",
      "Epoch 32 of 400\n",
      "Train Loss: 39469.2089\n",
      "Val Loss: 35811.7656\n",
      "Epoch 33 of 400\n",
      "Train Loss: 39451.0805\n",
      "Val Loss: 35705.4717\n",
      "Epoch 34 of 400\n",
      "Train Loss: 39440.6179\n",
      "Val Loss: 35838.3608\n",
      "Epoch 35 of 400\n",
      "Train Loss: 39430.2198\n",
      "Val Loss: 35754.5280\n",
      "Epoch 36 of 400\n",
      "Train Loss: 39435.0573\n",
      "Val Loss: 35801.7188\n",
      "Epoch 37 of 400\n",
      "Train Loss: 39434.6266\n",
      "Val Loss: 36016.7586\n",
      "Epoch 38 of 400\n",
      "Train Loss: 39466.6458\n",
      "Val Loss: 35758.4215\n",
      "Epoch 39 of 400\n",
      "Train Loss: 39349.7313\n",
      "Val Loss: 35638.7789\n",
      "Epoch 40 of 400\n",
      "Train Loss: 39285.0455\n",
      "Val Loss: 35552.8307\n",
      "Epoch 41 of 400\n",
      "Train Loss: 39273.5187\n",
      "Val Loss: 35801.0671\n",
      "Epoch 42 of 400\n",
      "Train Loss: 39284.4447\n",
      "Val Loss: 35569.4451\n",
      "Epoch 43 of 400\n",
      "Train Loss: 39209.1434\n",
      "Val Loss: 35501.2003\n",
      "Epoch 44 of 400\n",
      "Train Loss: 39192.6323\n",
      "Val Loss: 35568.4019\n",
      "Epoch 45 of 400\n",
      "Train Loss: 39179.5150\n",
      "Val Loss: 35501.5275\n",
      "Epoch 46 of 400\n",
      "Train Loss: 39170.4877\n",
      "Val Loss: 35533.0992\n",
      "Epoch 47 of 400\n",
      "Train Loss: 39167.0523\n",
      "Val Loss: 35469.5751\n",
      "Epoch 48 of 400\n",
      "Train Loss: 39125.0041\n",
      "Val Loss: 35434.1536\n",
      "Epoch 49 of 400\n",
      "Train Loss: 39111.5420\n",
      "Val Loss: 35387.9510\n",
      "Epoch 50 of 400\n",
      "Train Loss: 39062.1134\n",
      "Val Loss: 35387.5958\n",
      "Epoch 51 of 400\n",
      "Train Loss: 39058.7549\n",
      "Val Loss: 35356.3661\n",
      "Epoch 52 of 400\n",
      "Train Loss: 39058.3432\n",
      "Val Loss: 35457.9598\n",
      "Epoch 53 of 400\n",
      "Train Loss: 39056.0362\n",
      "Val Loss: 35378.3232\n",
      "Epoch 54 of 400\n",
      "Train Loss: 39026.3897\n",
      "Val Loss: 35321.6313\n",
      "Epoch 55 of 400\n",
      "Train Loss: 39005.4298\n",
      "Val Loss: 35325.1682\n",
      "Epoch 56 of 400\n",
      "Train Loss: 38972.7274\n",
      "Val Loss: 35309.6124\n",
      "Epoch 57 of 400\n",
      "Train Loss: 39025.9589\n",
      "Val Loss: 35296.9506\n",
      "Epoch 58 of 400\n",
      "Train Loss: 38977.2622\n",
      "Val Loss: 35307.4456\n",
      "Epoch 59 of 400\n",
      "Train Loss: 38933.0995\n",
      "Val Loss: 35278.2854\n",
      "Epoch 60 of 400\n",
      "Train Loss: 38941.8420\n",
      "Val Loss: 35304.5341\n",
      "Epoch 61 of 400\n",
      "Train Loss: 38933.6015\n",
      "Val Loss: 35286.3052\n",
      "Epoch 62 of 400\n",
      "Train Loss: 38905.5122\n",
      "Val Loss: 35250.5465\n",
      "Epoch 63 of 400\n",
      "Train Loss: 38908.0008\n",
      "Val Loss: 35321.6006\n",
      "Epoch 64 of 400\n",
      "Train Loss: 38927.7856\n",
      "Val Loss: 35301.1591\n",
      "Epoch 65 of 400\n",
      "Train Loss: 38910.1837\n",
      "Val Loss: 35287.8492\n",
      "Epoch 66 of 400\n",
      "Train Loss: 38853.2058\n",
      "Val Loss: 35233.2724\n",
      "Epoch 67 of 400\n",
      "Train Loss: 38871.6209\n",
      "Val Loss: 35235.5002\n",
      "Epoch 68 of 400\n",
      "Train Loss: 38853.8637\n",
      "Val Loss: 35241.2560\n",
      "Epoch 69 of 400\n",
      "Train Loss: 38818.5529\n",
      "Val Loss: 35228.1936\n",
      "Epoch 70 of 400\n",
      "Train Loss: 38829.7495\n",
      "Val Loss: 35174.4791\n",
      "Epoch 71 of 400\n",
      "Train Loss: 38834.1947\n",
      "Val Loss: 35191.1985\n",
      "Epoch 72 of 400\n",
      "Train Loss: 38813.6066\n",
      "Val Loss: 35169.4268\n",
      "Epoch 73 of 400\n",
      "Train Loss: 38795.5729\n",
      "Val Loss: 35152.1270\n",
      "Epoch 74 of 400\n",
      "Train Loss: 38779.6827\n",
      "Val Loss: 35144.7789\n",
      "Epoch 75 of 400\n",
      "Train Loss: 38771.4120\n",
      "Val Loss: 35162.4216\n",
      "Epoch 76 of 400\n",
      "Train Loss: 38764.3341\n",
      "Val Loss: 35180.8197\n",
      "Epoch 77 of 400\n",
      "Train Loss: 38753.2677\n",
      "Val Loss: 35143.5023\n",
      "Epoch 78 of 400\n",
      "Train Loss: 38778.6225\n",
      "Val Loss: 35149.3170\n",
      "Epoch 79 of 400\n",
      "Train Loss: 38743.3437\n",
      "Val Loss: 35130.4773\n",
      "Epoch 80 of 400\n",
      "Train Loss: 38725.8853\n",
      "Val Loss: 35106.2535\n",
      "Epoch 81 of 400\n",
      "Train Loss: 38697.0417\n",
      "Val Loss: 35094.3885\n",
      "Epoch 82 of 400\n",
      "Train Loss: 38715.7679\n",
      "Val Loss: 35139.6999\n",
      "Epoch 83 of 400\n",
      "Train Loss: 38726.5841\n",
      "Val Loss: 35089.4736\n",
      "Epoch 84 of 400\n",
      "Train Loss: 38683.6046\n",
      "Val Loss: 35081.6410\n",
      "Epoch 85 of 400\n",
      "Train Loss: 38662.7628\n",
      "Val Loss: 35050.3062\n",
      "Epoch 86 of 400\n",
      "Train Loss: 38669.9391\n",
      "Val Loss: 35042.4417\n",
      "Epoch 87 of 400\n",
      "Train Loss: 38662.4885\n",
      "Val Loss: 35146.8617\n",
      "Epoch 88 of 400\n",
      "Train Loss: 38679.1162\n",
      "Val Loss: 35097.6704\n",
      "Epoch 89 of 400\n",
      "Train Loss: 38660.2675\n",
      "Val Loss: 35055.0721\n",
      "Epoch 90 of 400\n",
      "Train Loss: 38608.5380\n",
      "Val Loss: 35010.9766\n",
      "Epoch 91 of 400\n",
      "Train Loss: 38610.4136\n",
      "Val Loss: 35080.1110\n",
      "Epoch 92 of 400\n",
      "Train Loss: 38621.7679\n",
      "Val Loss: 35002.6488\n",
      "Epoch 93 of 400\n",
      "Train Loss: 38602.2859\n",
      "Val Loss: 35086.4060\n",
      "Epoch 94 of 400\n",
      "Train Loss: 38644.6848\n",
      "Val Loss: 35006.6556\n",
      "Epoch 95 of 400\n",
      "Train Loss: 38566.2453\n",
      "Val Loss: 34987.1993\n",
      "Epoch 96 of 400\n",
      "Train Loss: 38570.9132\n",
      "Val Loss: 35028.0215\n",
      "Epoch 97 of 400\n",
      "Train Loss: 38564.7478\n",
      "Val Loss: 35009.2107\n",
      "Epoch 98 of 400\n",
      "Train Loss: 38543.8655\n",
      "Val Loss: 34969.4879\n",
      "Epoch 99 of 400\n",
      "Train Loss: 38522.3534\n",
      "Val Loss: 34975.4071\n",
      "Epoch 100 of 400\n",
      "Train Loss: 38546.7317\n",
      "Val Loss: 34935.7657\n",
      "Epoch 101 of 400\n",
      "Train Loss: 38510.9054\n",
      "Val Loss: 34955.1665\n",
      "Epoch 102 of 400\n",
      "Train Loss: 38495.4423\n",
      "Val Loss: 34935.0774\n",
      "Epoch 103 of 400\n",
      "Train Loss: 38499.0276\n",
      "Val Loss: 34900.7340\n",
      "Epoch 104 of 400\n",
      "Train Loss: 38461.6984\n",
      "Val Loss: 34909.3530\n",
      "Epoch 105 of 400\n",
      "Train Loss: 38473.3201\n",
      "Val Loss: 34877.1084\n",
      "Epoch 106 of 400\n",
      "Train Loss: 38457.3276\n",
      "Val Loss: 34968.6473\n",
      "Epoch 107 of 400\n",
      "Train Loss: 38475.8694\n",
      "Val Loss: 34840.5657\n",
      "Epoch 108 of 400\n",
      "Train Loss: 38420.6647\n",
      "Val Loss: 34864.6785\n",
      "Epoch 109 of 400\n",
      "Train Loss: 38408.0149\n",
      "Val Loss: 34838.9911\n",
      "Epoch 110 of 400\n",
      "Train Loss: 38410.8110\n",
      "Val Loss: 34846.6606\n",
      "Epoch 111 of 400\n",
      "Train Loss: 38387.7609\n",
      "Val Loss: 34806.7669\n",
      "Epoch 112 of 400\n",
      "Train Loss: 38387.9816\n",
      "Val Loss: 34835.7640\n",
      "Epoch 113 of 400\n",
      "Train Loss: 38345.6982\n",
      "Val Loss: 34770.0619\n",
      "Epoch 114 of 400\n",
      "Train Loss: 38335.1110\n",
      "Val Loss: 34796.6169\n",
      "Epoch 115 of 400\n",
      "Train Loss: 38390.5227\n",
      "Val Loss: 34777.4795\n",
      "Epoch 116 of 400\n",
      "Train Loss: 38354.6353\n",
      "Val Loss: 34787.7261\n",
      "Epoch 117 of 400\n",
      "Train Loss: 38338.3582\n",
      "Val Loss: 34888.9750\n",
      "Epoch 118 of 400\n",
      "Train Loss: 38329.1541\n",
      "Val Loss: 34767.9410\n",
      "Epoch 119 of 400\n",
      "Train Loss: 38311.7709\n",
      "Val Loss: 34784.7793\n",
      "Epoch 120 of 400\n",
      "Train Loss: 38308.3159\n",
      "Val Loss: 34793.8926\n",
      "Epoch 121 of 400\n",
      "Train Loss: 38343.0836\n",
      "Val Loss: 34773.9737\n",
      "Epoch 122 of 400\n",
      "Train Loss: 38298.2462\n",
      "Val Loss: 34847.4927\n",
      "Epoch 123 of 400\n",
      "Train Loss: 38303.1024\n",
      "Val Loss: 34710.8928\n",
      "Epoch 124 of 400\n",
      "Train Loss: 38294.2654\n",
      "Val Loss: 34739.3671\n",
      "Epoch 125 of 400\n",
      "Train Loss: 38293.0977\n",
      "Val Loss: 34869.4577\n",
      "Epoch 126 of 400\n",
      "Train Loss: 38271.3148\n",
      "Val Loss: 34759.6561\n",
      "Epoch 127 of 400\n",
      "Train Loss: 38261.2180\n",
      "Val Loss: 34866.8546\n",
      "Epoch 128 of 400\n",
      "Train Loss: 38280.6433\n",
      "Val Loss: 34854.9997\n",
      "Epoch 129 of 400\n",
      "Train Loss: 38287.0917\n",
      "Val Loss: 34873.4969\n",
      "Epoch 130 of 400\n",
      "Train Loss: 38300.2380\n",
      "Val Loss: 34777.7204\n",
      "Epoch 131 of 400\n",
      "Train Loss: 38254.4538\n",
      "Val Loss: 34756.4600\n",
      "Epoch 132 of 400\n",
      "Train Loss: 38212.6082\n",
      "Val Loss: 34650.1438\n",
      "Epoch 133 of 400\n",
      "Train Loss: 38207.1089\n",
      "Val Loss: 34702.6503\n",
      "Epoch 134 of 400\n",
      "Train Loss: 38230.8607\n",
      "Val Loss: 34759.6048\n",
      "Epoch 135 of 400\n",
      "Train Loss: 38211.5523\n",
      "Val Loss: 34668.2577\n",
      "Epoch 136 of 400\n",
      "Train Loss: 38174.7541\n",
      "Val Loss: 34679.6820\n",
      "Epoch 137 of 400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 38209.6139\n",
      "Val Loss: 34690.8713\n",
      "Epoch 138 of 400\n",
      "Train Loss: 38182.4288\n",
      "Val Loss: 34670.2702\n",
      "Epoch 139 of 400\n",
      "Train Loss: 38161.8348\n",
      "Val Loss: 34611.5070\n",
      "Epoch 140 of 400\n",
      "Train Loss: 38153.2119\n",
      "Val Loss: 34652.6387\n",
      "Epoch 141 of 400\n",
      "Train Loss: 38162.0084\n",
      "Val Loss: 34681.0231\n",
      "Epoch 142 of 400\n",
      "Train Loss: 38136.9895\n",
      "Val Loss: 34614.3428\n",
      "Epoch 143 of 400\n",
      "Train Loss: 38129.7736\n",
      "Val Loss: 34627.1338\n",
      "Epoch 144 of 400\n",
      "Train Loss: 38127.0196\n",
      "Val Loss: 34642.0388\n",
      "Epoch 145 of 400\n",
      "Train Loss: 38129.3283\n",
      "Val Loss: 34607.2219\n",
      "Epoch 146 of 400\n",
      "Train Loss: 38111.3891\n",
      "Val Loss: 34602.5160\n",
      "Epoch 147 of 400\n",
      "Train Loss: 38111.4948\n",
      "Val Loss: 34666.3322\n",
      "Epoch 148 of 400\n",
      "Train Loss: 38135.8384\n",
      "Val Loss: 34627.8645\n",
      "Epoch 149 of 400\n",
      "Train Loss: 38126.4484\n",
      "Val Loss: 34672.0613\n",
      "Epoch 150 of 400\n",
      "Train Loss: 38212.2952\n",
      "Val Loss: 34743.8201\n",
      "Epoch 151 of 400\n",
      "Train Loss: 38143.7029\n",
      "Val Loss: 34592.8135\n",
      "Epoch 152 of 400\n",
      "Train Loss: 38090.6275\n",
      "Val Loss: 34589.3014\n",
      "Epoch 153 of 400\n",
      "Train Loss: 38095.9833\n",
      "Val Loss: 34594.8106\n",
      "Epoch 154 of 400\n",
      "Train Loss: 38118.2706\n",
      "Val Loss: 34596.3687\n",
      "Epoch 155 of 400\n",
      "Train Loss: 38064.8732\n",
      "Val Loss: 34559.8102\n",
      "Epoch 156 of 400\n",
      "Train Loss: 38084.8252\n",
      "Val Loss: 34560.3218\n",
      "Epoch 157 of 400\n",
      "Train Loss: 38064.2970\n",
      "Val Loss: 34637.7907\n",
      "Epoch 158 of 400\n",
      "Train Loss: 38092.6837\n",
      "Val Loss: 34702.7727\n",
      "Epoch 159 of 400\n",
      "Train Loss: 38045.0498\n",
      "Val Loss: 34543.9591\n",
      "Epoch 160 of 400\n",
      "Train Loss: 38021.4333\n",
      "Val Loss: 34573.7181\n",
      "Epoch 161 of 400\n",
      "Train Loss: 38048.5497\n",
      "Val Loss: 34644.7445\n",
      "Epoch 162 of 400\n",
      "Train Loss: 38068.1368\n",
      "Val Loss: 34575.1743\n",
      "Epoch 163 of 400\n",
      "Train Loss: 38107.2721\n",
      "Val Loss: 34759.6097\n",
      "Epoch 164 of 400\n",
      "Train Loss: 38085.8694\n",
      "Val Loss: 34594.1467\n",
      "Epoch 165 of 400\n",
      "Train Loss: 38028.2177\n",
      "Val Loss: 34626.6908\n",
      "Epoch 166 of 400\n",
      "Train Loss: 38027.0414\n",
      "Val Loss: 34546.4415\n",
      "Epoch 167 of 400\n",
      "Train Loss: 38019.6648\n",
      "Val Loss: 34541.5231\n",
      "Epoch 168 of 400\n",
      "Train Loss: 37996.3516\n",
      "Val Loss: 34535.5271\n",
      "Epoch 169 of 400\n",
      "Train Loss: 37996.1421\n",
      "Val Loss: 34499.4181\n",
      "Epoch 170 of 400\n",
      "Train Loss: 38028.6883\n",
      "Val Loss: 34551.0615\n",
      "Epoch 171 of 400\n",
      "Train Loss: 37993.7893\n",
      "Val Loss: 34579.3029\n",
      "Epoch 172 of 400\n",
      "Train Loss: 37970.6793\n",
      "Val Loss: 34511.9023\n",
      "Epoch 173 of 400\n",
      "Train Loss: 37964.7185\n",
      "Val Loss: 34574.1307\n",
      "Epoch 174 of 400\n",
      "Train Loss: 38018.5729\n",
      "Val Loss: 34636.0621\n",
      "Epoch 175 of 400\n",
      "Train Loss: 37986.3487\n",
      "Val Loss: 34544.6814\n",
      "Epoch 176 of 400\n",
      "Train Loss: 37973.2616\n",
      "Val Loss: 34593.0430\n",
      "Epoch 177 of 400\n",
      "Train Loss: 37998.9864\n",
      "Val Loss: 34526.6436\n",
      "Epoch 178 of 400\n",
      "Train Loss: 37944.1300\n",
      "Val Loss: 34473.8970\n",
      "Epoch 179 of 400\n",
      "Train Loss: 37940.9705\n",
      "Val Loss: 34483.0974\n",
      "Epoch 180 of 400\n",
      "Train Loss: 37922.6729\n",
      "Val Loss: 34512.5507\n",
      "Epoch 181 of 400\n",
      "Train Loss: 37960.8446\n",
      "Val Loss: 34539.0581\n",
      "Epoch 182 of 400\n",
      "Train Loss: 37997.2985\n",
      "Val Loss: 34471.1848\n",
      "Epoch 183 of 400\n",
      "Train Loss: 37973.7945\n",
      "Val Loss: 34596.8385\n",
      "Epoch 184 of 400\n",
      "Train Loss: 37989.6103\n",
      "Val Loss: 34617.4885\n",
      "Epoch 185 of 400\n",
      "Train Loss: 38161.1535\n",
      "Val Loss: 34666.7815\n",
      "Epoch 186 of 400\n",
      "Train Loss: 37999.5144\n",
      "Val Loss: 34528.4085\n",
      "Epoch 187 of 400\n",
      "Train Loss: 37934.5185\n",
      "Val Loss: 34479.1415\n",
      "Epoch 188 of 400\n",
      "Train Loss: 37930.7285\n",
      "Val Loss: 34508.1046\n",
      "Epoch 189 of 400\n",
      "Train Loss: 37916.1518\n",
      "Val Loss: 34455.7883\n",
      "Epoch 190 of 400\n",
      "Train Loss: 37896.1890\n",
      "Val Loss: 34452.9177\n",
      "Epoch 191 of 400\n",
      "Train Loss: 37906.7758\n",
      "Val Loss: 34480.3910\n",
      "Epoch 192 of 400\n",
      "Train Loss: 37922.4975\n",
      "Val Loss: 34498.0422\n",
      "Epoch 193 of 400\n",
      "Train Loss: 37938.7518\n",
      "Val Loss: 34627.7022\n",
      "Epoch 194 of 400\n",
      "Train Loss: 38004.7978\n",
      "Val Loss: 34492.5004\n",
      "Epoch 195 of 400\n",
      "Train Loss: 37891.8409\n",
      "Val Loss: 34472.0940\n",
      "Epoch 196 of 400\n",
      "Train Loss: 37898.5660\n",
      "Val Loss: 34419.9143\n",
      "Epoch 197 of 400\n",
      "Train Loss: 37873.2739\n",
      "Val Loss: 34474.3429\n",
      "Epoch 198 of 400\n",
      "Train Loss: 37887.2176\n",
      "Val Loss: 34458.1865\n",
      "Epoch 199 of 400\n",
      "Train Loss: 37947.6578\n",
      "Val Loss: 34420.8567\n",
      "Epoch 200 of 400\n",
      "Train Loss: 37873.1148\n",
      "Val Loss: 34406.8615\n",
      "Epoch 201 of 400\n",
      "Train Loss: 37850.3793\n",
      "Val Loss: 34418.1335\n",
      "Epoch 202 of 400\n",
      "Train Loss: 37845.0930\n",
      "Val Loss: 34439.0891\n",
      "Epoch 203 of 400\n",
      "Train Loss: 37843.1632\n",
      "Val Loss: 34407.3586\n",
      "Epoch 204 of 400\n",
      "Train Loss: 37984.7986\n",
      "Val Loss: 34768.2574\n",
      "Epoch 205 of 400\n",
      "Train Loss: 38032.9006\n",
      "Val Loss: 34530.4199\n",
      "Epoch 206 of 400\n",
      "Train Loss: 37884.2160\n",
      "Val Loss: 34426.6217\n",
      "Epoch 207 of 400\n",
      "Train Loss: 37844.1156\n",
      "Val Loss: 34397.7713\n",
      "Epoch 208 of 400\n",
      "Train Loss: 37845.6277\n",
      "Val Loss: 34393.7003\n",
      "Epoch 209 of 400\n",
      "Train Loss: 37831.7106\n",
      "Val Loss: 34438.0771\n",
      "Epoch 210 of 400\n",
      "Train Loss: 37855.5373\n",
      "Val Loss: 34376.1428\n",
      "Epoch 211 of 400\n",
      "Train Loss: 37814.7850\n",
      "Val Loss: 34442.8433\n",
      "Epoch 212 of 400\n",
      "Train Loss: 37845.7100\n",
      "Val Loss: 34398.9378\n",
      "Epoch 213 of 400\n",
      "Train Loss: 37812.6323\n",
      "Val Loss: 34370.6028\n",
      "Epoch 214 of 400\n",
      "Train Loss: 37834.1720\n",
      "Val Loss: 34369.3042\n",
      "Epoch 215 of 400\n",
      "Train Loss: 37808.3502\n",
      "Val Loss: 34423.8823\n",
      "Epoch 216 of 400\n",
      "Train Loss: 37839.0134\n",
      "Val Loss: 34380.4920\n",
      "Epoch 217 of 400\n",
      "Train Loss: 37801.6146\n",
      "Val Loss: 34367.8994\n",
      "Epoch 218 of 400\n",
      "Train Loss: 37806.0410\n",
      "Val Loss: 34402.9517\n",
      "Epoch 219 of 400\n",
      "Train Loss: 37802.3720\n",
      "Val Loss: 34339.6636\n",
      "Epoch 220 of 400\n",
      "Train Loss: 37798.5053\n",
      "Val Loss: 34362.3488\n",
      "Epoch 221 of 400\n",
      "Train Loss: 37808.6321\n",
      "Val Loss: 34361.8878\n",
      "Epoch 222 of 400\n",
      "Train Loss: 37803.1946\n",
      "Val Loss: 34453.7713\n",
      "Epoch 223 of 400\n",
      "Train Loss: 37829.4073\n",
      "Val Loss: 34356.7122\n",
      "Epoch 224 of 400\n",
      "Train Loss: 37800.1972\n",
      "Val Loss: 34371.5308\n",
      "Epoch 225 of 400\n",
      "Train Loss: 37808.4341\n",
      "Val Loss: 34381.4395\n",
      "Epoch 226 of 400\n",
      "Train Loss: 37865.3908\n",
      "Val Loss: 34486.0970\n",
      "Epoch 227 of 400\n",
      "Train Loss: 37972.5958\n",
      "Val Loss: 34438.7247\n",
      "Epoch 228 of 400\n",
      "Train Loss: 37855.2218\n",
      "Val Loss: 34377.8689\n",
      "Epoch 229 of 400\n",
      "Train Loss: 37785.8693\n",
      "Val Loss: 34383.6761\n",
      "Epoch 230 of 400\n",
      "Train Loss: 37795.7876\n",
      "Val Loss: 34351.9199\n",
      "Epoch 231 of 400\n",
      "Train Loss: 37764.2797\n",
      "Val Loss: 34347.9572\n",
      "Epoch 232 of 400\n",
      "Train Loss: 37757.3642\n",
      "Val Loss: 34308.8567\n",
      "Epoch 233 of 400\n",
      "Train Loss: 37744.5284\n",
      "Val Loss: 34337.1902\n",
      "Epoch 234 of 400\n",
      "Train Loss: 37746.1266\n",
      "Val Loss: 34320.5215\n",
      "Epoch 235 of 400\n",
      "Train Loss: 37755.2481\n",
      "Val Loss: 34359.0661\n",
      "Epoch 236 of 400\n",
      "Train Loss: 37760.3173\n",
      "Val Loss: 34347.7999\n",
      "Epoch 237 of 400\n",
      "Train Loss: 37744.8194\n",
      "Val Loss: 34303.7789\n",
      "Epoch 238 of 400\n",
      "Train Loss: 37743.9297\n",
      "Val Loss: 34361.0871\n",
      "Epoch 239 of 400\n",
      "Train Loss: 37769.1562\n",
      "Val Loss: 34347.2301\n",
      "Epoch 240 of 400\n",
      "Train Loss: 37742.5620\n",
      "Val Loss: 34319.4153\n",
      "Epoch 241 of 400\n",
      "Train Loss: 37735.2734\n",
      "Val Loss: 34342.0388\n",
      "Epoch 242 of 400\n",
      "Train Loss: 37739.6483\n",
      "Val Loss: 34392.5492\n",
      "Epoch 243 of 400\n",
      "Train Loss: 37793.8058\n",
      "Val Loss: 34292.7183\n",
      "Epoch 244 of 400\n",
      "Train Loss: 37726.3681\n",
      "Val Loss: 34314.2828\n",
      "Epoch 245 of 400\n",
      "Train Loss: 37725.1641\n",
      "Val Loss: 34332.1352\n",
      "Epoch 246 of 400\n",
      "Train Loss: 37715.8154\n",
      "Val Loss: 34299.3463\n",
      "Epoch 247 of 400\n",
      "Train Loss: 37733.7014\n",
      "Val Loss: 34303.1982\n",
      "Epoch 248 of 400\n",
      "Train Loss: 37731.6861\n",
      "Val Loss: 34294.1313\n",
      "Epoch 249 of 400\n",
      "Train Loss: 37711.5016\n",
      "Val Loss: 34304.9078\n",
      "Epoch 250 of 400\n",
      "Train Loss: 37708.1523\n",
      "Val Loss: 34263.5139\n",
      "Epoch 251 of 400\n",
      "Train Loss: 37698.7927\n",
      "Val Loss: 34284.0030\n",
      "Epoch 252 of 400\n",
      "Train Loss: 37743.8877\n",
      "Val Loss: 34404.1311\n",
      "Epoch 253 of 400\n",
      "Train Loss: 37801.6412\n",
      "Val Loss: 34512.5902\n",
      "Epoch 254 of 400\n",
      "Train Loss: 37796.2490\n",
      "Val Loss: 34284.4344\n",
      "Epoch 255 of 400\n",
      "Train Loss: 37712.0427\n",
      "Val Loss: 34300.1287\n",
      "Epoch 256 of 400\n",
      "Train Loss: 37720.7908\n",
      "Val Loss: 34291.2889\n",
      "Epoch 257 of 400\n",
      "Train Loss: 37705.4034\n",
      "Val Loss: 34277.3865\n",
      "Epoch 258 of 400\n",
      "Train Loss: 37753.9561\n",
      "Val Loss: 34497.7973\n",
      "Epoch 259 of 400\n",
      "Train Loss: 37726.6962\n",
      "Val Loss: 34263.2600\n",
      "Epoch 260 of 400\n",
      "Train Loss: 37686.6602\n",
      "Val Loss: 34287.6962\n",
      "Epoch 261 of 400\n",
      "Train Loss: 37715.4721\n",
      "Val Loss: 34311.4902\n",
      "Epoch 262 of 400\n",
      "Train Loss: 37694.6223\n",
      "Val Loss: 34268.7573\n",
      "Epoch 263 of 400\n",
      "Train Loss: 37693.8790\n",
      "Val Loss: 34303.9007\n",
      "Epoch 264 of 400\n",
      "Train Loss: 37701.1426\n",
      "Val Loss: 34272.0772\n",
      "Epoch 265 of 400\n",
      "Train Loss: 37702.5405\n",
      "Val Loss: 34533.7671\n",
      "Epoch 266 of 400\n",
      "Train Loss: 37733.5944\n",
      "Val Loss: 34278.4051\n",
      "Epoch 267 of 400\n",
      "Train Loss: 37691.2390\n",
      "Val Loss: 34289.1175\n",
      "Epoch 268 of 400\n",
      "Train Loss: 37673.3162\n",
      "Val Loss: 34228.1316\n",
      "Epoch 269 of 400\n",
      "Train Loss: 37677.0680\n",
      "Val Loss: 34273.1156\n",
      "Epoch 270 of 400\n",
      "Train Loss: 37674.8138\n",
      "Val Loss: 34271.1202\n",
      "Epoch 271 of 400\n",
      "Train Loss: 37697.4437\n",
      "Val Loss: 34275.6410\n",
      "Epoch 272 of 400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 37668.1523\n",
      "Val Loss: 34267.9360\n",
      "Epoch 273 of 400\n",
      "Train Loss: 37670.9458\n",
      "Val Loss: 34248.0497\n",
      "Epoch 274 of 400\n",
      "Train Loss: 37700.4326\n",
      "Val Loss: 34292.3772\n",
      "Epoch 275 of 400\n",
      "Train Loss: 37694.7177\n",
      "Val Loss: 34305.6353\n",
      "Epoch 276 of 400\n",
      "Train Loss: 37696.4196\n",
      "Val Loss: 34248.5877\n",
      "Epoch 277 of 400\n",
      "Train Loss: 37644.9848\n",
      "Val Loss: 34252.3278\n",
      "Epoch 278 of 400\n",
      "Train Loss: 37655.3203\n",
      "Val Loss: 34270.8743\n",
      "Epoch 279 of 400\n",
      "Train Loss: 37676.7425\n",
      "Val Loss: 34273.2317\n",
      "Epoch 280 of 400\n",
      "Train Loss: 37646.1010\n",
      "Val Loss: 34307.2130\n",
      "Epoch 281 of 400\n",
      "Train Loss: 37675.3850\n",
      "Val Loss: 34239.9688\n",
      "Epoch 282 of 400\n",
      "Train Loss: 37643.4493\n",
      "Val Loss: 34217.8989\n",
      "Epoch 283 of 400\n",
      "Train Loss: 37633.8985\n",
      "Val Loss: 34296.0529\n",
      "Epoch 284 of 400\n",
      "Train Loss: 37638.9750\n",
      "Val Loss: 34223.6305\n",
      "Epoch 285 of 400\n",
      "Train Loss: 37641.5736\n",
      "Val Loss: 34250.0888\n",
      "Epoch 286 of 400\n",
      "Train Loss: 37644.5320\n",
      "Val Loss: 34232.2985\n",
      "Epoch 287 of 400\n",
      "Train Loss: 37610.9265\n",
      "Val Loss: 34204.6076\n",
      "Epoch 288 of 400\n",
      "Train Loss: 37631.0116\n",
      "Val Loss: 34315.8353\n",
      "Epoch 289 of 400\n",
      "Train Loss: 37635.4648\n",
      "Val Loss: 34228.9679\n",
      "Epoch 290 of 400\n",
      "Train Loss: 37678.5929\n",
      "Val Loss: 34245.8149\n",
      "Epoch 291 of 400\n",
      "Train Loss: 37650.6427\n",
      "Val Loss: 34342.5778\n",
      "Epoch 292 of 400\n",
      "Train Loss: 37655.3074\n",
      "Val Loss: 34224.8247\n",
      "Epoch 293 of 400\n",
      "Train Loss: 37636.1270\n",
      "Val Loss: 34258.2441\n",
      "Epoch 294 of 400\n",
      "Train Loss: 37606.6871\n",
      "Val Loss: 34214.9258\n",
      "Epoch 295 of 400\n",
      "Train Loss: 37615.8562\n",
      "Val Loss: 34272.2548\n",
      "Epoch 296 of 400\n",
      "Train Loss: 37646.8684\n",
      "Val Loss: 34280.6746\n",
      "Epoch 297 of 400\n",
      "Train Loss: 37627.8572\n",
      "Val Loss: 34236.4501\n",
      "Epoch 298 of 400\n",
      "Train Loss: 37633.8761\n",
      "Val Loss: 34195.0165\n",
      "Epoch 299 of 400\n",
      "Train Loss: 37614.0276\n",
      "Val Loss: 34213.9412\n",
      "Epoch 300 of 400\n",
      "Train Loss: 37628.0269\n",
      "Val Loss: 34296.2808\n",
      "Epoch 301 of 400\n",
      "Train Loss: 37607.6162\n",
      "Val Loss: 34202.4520\n",
      "Epoch 302 of 400\n",
      "Train Loss: 37642.2923\n",
      "Val Loss: 34452.6794\n",
      "Epoch 303 of 400\n",
      "Train Loss: 37962.4215\n",
      "Val Loss: 34628.3230\n",
      "Epoch 304 of 400\n",
      "Train Loss: 37790.9566\n",
      "Val Loss: 34260.1909\n",
      "Epoch 305 of 400\n",
      "Train Loss: 37631.3011\n",
      "Val Loss: 34219.0759\n",
      "Epoch 306 of 400\n",
      "Train Loss: 37624.3764\n",
      "Val Loss: 34202.2084\n",
      "Epoch 307 of 400\n",
      "Train Loss: 37591.8621\n",
      "Val Loss: 34212.2718\n",
      "Epoch 308 of 400\n",
      "Train Loss: 37578.2320\n",
      "Val Loss: 34183.3081\n",
      "Epoch 309 of 400\n",
      "Train Loss: 37585.9114\n",
      "Val Loss: 34186.9005\n",
      "Epoch 310 of 400\n",
      "Train Loss: 37585.7171\n",
      "Val Loss: 34279.6279\n",
      "Epoch 311 of 400\n",
      "Train Loss: 37597.9599\n",
      "Val Loss: 34282.1666\n",
      "Epoch 312 of 400\n",
      "Train Loss: 37598.4610\n",
      "Val Loss: 34208.1364\n",
      "Epoch 313 of 400\n",
      "Train Loss: 37583.8394\n",
      "Val Loss: 34207.7544\n",
      "Epoch 314 of 400\n",
      "Train Loss: 37590.7106\n",
      "Val Loss: 34174.6618\n",
      "Epoch 315 of 400\n",
      "Train Loss: 37564.3295\n",
      "Val Loss: 34181.0110\n",
      "Epoch 316 of 400\n",
      "Train Loss: 37571.9374\n",
      "Val Loss: 34200.2859\n",
      "Epoch 317 of 400\n",
      "Train Loss: 37564.9997\n",
      "Val Loss: 34167.7291\n",
      "Epoch 318 of 400\n",
      "Train Loss: 37569.7562\n",
      "Val Loss: 34238.2077\n",
      "Epoch 319 of 400\n",
      "Train Loss: 37586.5995\n",
      "Val Loss: 34208.5706\n",
      "Epoch 320 of 400\n",
      "Train Loss: 37564.1200\n",
      "Val Loss: 34226.7837\n",
      "Epoch 321 of 400\n",
      "Train Loss: 37576.2716\n",
      "Val Loss: 34204.9355\n",
      "Epoch 322 of 400\n",
      "Train Loss: 37581.3378\n",
      "Val Loss: 34371.2194\n",
      "Epoch 323 of 400\n",
      "Train Loss: 37653.8534\n",
      "Val Loss: 34202.8426\n",
      "Epoch 324 of 400\n",
      "Train Loss: 37597.6010\n",
      "Val Loss: 34221.9449\n",
      "Epoch 325 of 400\n",
      "Train Loss: 37573.3453\n",
      "Val Loss: 34188.4694\n",
      "Epoch 326 of 400\n",
      "Train Loss: 37545.6301\n",
      "Val Loss: 34184.9695\n",
      "Epoch 327 of 400\n",
      "Train Loss: 37553.9181\n",
      "Val Loss: 34168.3747\n",
      "Epoch 328 of 400\n",
      "Train Loss: 37573.8355\n",
      "Val Loss: 34194.9716\n",
      "Epoch 329 of 400\n",
      "Train Loss: 37601.2584\n",
      "Val Loss: 34192.5850\n",
      "Epoch 330 of 400\n",
      "Train Loss: 37609.8791\n",
      "Val Loss: 34166.8316\n",
      "Epoch 331 of 400\n",
      "Train Loss: 37567.8995\n",
      "Val Loss: 34190.4406\n",
      "Epoch 332 of 400\n",
      "Train Loss: 37556.5191\n",
      "Val Loss: 34239.1040\n",
      "Epoch 333 of 400\n",
      "Train Loss: 37632.3268\n",
      "Val Loss: 34207.3458\n",
      "Epoch 334 of 400\n",
      "Train Loss: 37581.8487\n",
      "Val Loss: 34173.7709\n",
      "Epoch 335 of 400\n",
      "Train Loss: 37575.7524\n",
      "Val Loss: 34150.3075\n",
      "Epoch 336 of 400\n",
      "Train Loss: 37537.2812\n",
      "Val Loss: 34149.7871\n",
      "Epoch 337 of 400\n",
      "Train Loss: 37557.8888\n",
      "Val Loss: 34162.3142\n",
      "Epoch 338 of 400\n",
      "Train Loss: 37558.9219\n",
      "Val Loss: 34231.4971\n",
      "Epoch 339 of 400\n",
      "Train Loss: 37584.6551\n",
      "Val Loss: 34159.2620\n",
      "Epoch 340 of 400\n",
      "Train Loss: 37606.9241\n",
      "Val Loss: 34226.8998\n",
      "Epoch 341 of 400\n",
      "Train Loss: 37580.7128\n",
      "Val Loss: 34202.8953\n",
      "Epoch 342 of 400\n",
      "Train Loss: 37551.8202\n",
      "Val Loss: 34150.9844\n",
      "Epoch 343 of 400\n",
      "Train Loss: 37554.8420\n",
      "Val Loss: 34159.2257\n",
      "Epoch 344 of 400\n",
      "Train Loss: 37532.4845\n",
      "Val Loss: 34148.9614\n",
      "Epoch 345 of 400\n",
      "Train Loss: 37544.0727\n",
      "Val Loss: 34154.5879\n",
      "Epoch 346 of 400\n",
      "Train Loss: 37549.7726\n",
      "Val Loss: 34176.7135\n",
      "Epoch 347 of 400\n",
      "Train Loss: 37553.9878\n",
      "Val Loss: 34161.4842\n",
      "Epoch 348 of 400\n",
      "Train Loss: 37519.9724\n",
      "Val Loss: 34149.8918\n",
      "Epoch 349 of 400\n",
      "Train Loss: 37543.5045\n",
      "Val Loss: 34134.0582\n",
      "Epoch 350 of 400\n",
      "Train Loss: 37568.2563\n",
      "Val Loss: 34131.1327\n",
      "Epoch 351 of 400\n",
      "Train Loss: 37593.0474\n",
      "Val Loss: 34174.0339\n",
      "Epoch 352 of 400\n",
      "Train Loss: 37575.9012\n",
      "Val Loss: 34197.4082\n",
      "Epoch 353 of 400\n",
      "Train Loss: 37541.9262\n",
      "Val Loss: 34164.0622\n",
      "Epoch 354 of 400\n",
      "Train Loss: 37541.7522\n",
      "Val Loss: 34114.3080\n",
      "Epoch 355 of 400\n",
      "Train Loss: 37533.2366\n",
      "Val Loss: 34285.6560\n",
      "Epoch 356 of 400\n",
      "Train Loss: 37643.4553\n",
      "Val Loss: 34796.8200\n",
      "Epoch 357 of 400\n",
      "Train Loss: 37901.9009\n",
      "Val Loss: 34464.8979\n",
      "Epoch 358 of 400\n",
      "Train Loss: 37606.4540\n",
      "Val Loss: 34276.9994\n",
      "Epoch 359 of 400\n",
      "Train Loss: 37565.3137\n",
      "Val Loss: 34171.0414\n",
      "Epoch 360 of 400\n",
      "Train Loss: 37516.9552\n",
      "Val Loss: 34127.9629\n",
      "Epoch 361 of 400\n",
      "Train Loss: 37503.5636\n",
      "Val Loss: 34120.7134\n",
      "Epoch 362 of 400\n",
      "Train Loss: 37510.6132\n",
      "Val Loss: 34125.8834\n",
      "Epoch 363 of 400\n",
      "Train Loss: 37504.3466\n",
      "Val Loss: 34178.3329\n",
      "Epoch 364 of 400\n",
      "Train Loss: 37515.9863\n",
      "Val Loss: 34108.3908\n",
      "Epoch 365 of 400\n",
      "Train Loss: 37504.0672\n",
      "Val Loss: 34112.1890\n",
      "Epoch 366 of 400\n",
      "Train Loss: 37504.1305\n",
      "Val Loss: 34108.3880\n",
      "Epoch 367 of 400\n",
      "Train Loss: 37521.4560\n",
      "Val Loss: 34139.5301\n",
      "Epoch 368 of 400\n",
      "Train Loss: 37493.4233\n",
      "Val Loss: 34130.1030\n",
      "Epoch 369 of 400\n",
      "Train Loss: 37507.3820\n",
      "Val Loss: 34136.9171\n",
      "Epoch 370 of 400\n",
      "Train Loss: 37525.7812\n",
      "Val Loss: 34147.4171\n",
      "Epoch 371 of 400\n",
      "Train Loss: 37522.8475\n",
      "Val Loss: 34191.9699\n",
      "Epoch 372 of 400\n",
      "Train Loss: 37514.3340\n",
      "Val Loss: 34185.7977\n",
      "Epoch 373 of 400\n",
      "Train Loss: 37532.2407\n",
      "Val Loss: 34199.4850\n",
      "Epoch 374 of 400\n",
      "Train Loss: 37654.4781\n",
      "Val Loss: 34206.1212\n",
      "Epoch 375 of 400\n",
      "Train Loss: 37554.6401\n",
      "Val Loss: 34214.4599\n",
      "Epoch 376 of 400\n",
      "Train Loss: 37541.4460\n",
      "Val Loss: 34092.3380\n",
      "Epoch 377 of 400\n",
      "Train Loss: 37505.3566\n",
      "Val Loss: 34098.6393\n",
      "Epoch 378 of 400\n",
      "Train Loss: 37485.0258\n",
      "Val Loss: 34119.5190\n",
      "Epoch 379 of 400\n",
      "Train Loss: 37498.6030\n",
      "Val Loss: 34123.2880\n",
      "Epoch 380 of 400\n",
      "Train Loss: 37528.6724\n",
      "Val Loss: 34121.8261\n",
      "Epoch 381 of 400\n",
      "Train Loss: 37497.1679\n",
      "Val Loss: 34121.1588\n",
      "Epoch 382 of 400\n",
      "Train Loss: 37486.0292\n",
      "Val Loss: 34134.1269\n",
      "Epoch 383 of 400\n",
      "Train Loss: 37491.2798\n",
      "Val Loss: 34090.7510\n",
      "Epoch 384 of 400\n",
      "Train Loss: 37483.2845\n",
      "Val Loss: 34134.5617\n",
      "Epoch 385 of 400\n",
      "Train Loss: 37592.4237\n",
      "Val Loss: 34253.3207\n",
      "Epoch 386 of 400\n",
      "Train Loss: 37642.4651\n",
      "Val Loss: 34206.4675\n",
      "Epoch 387 of 400\n",
      "Train Loss: 37565.7791\n",
      "Val Loss: 34104.7672\n",
      "Epoch 388 of 400\n",
      "Train Loss: 37502.2780\n",
      "Val Loss: 34165.7950\n",
      "Epoch 389 of 400\n",
      "Train Loss: 37494.0872\n",
      "Val Loss: 34102.5193\n",
      "Epoch 390 of 400\n",
      "Train Loss: 37469.9273\n",
      "Val Loss: 34111.9714\n",
      "Epoch 391 of 400\n",
      "Train Loss: 37498.9514\n",
      "Val Loss: 34130.4042\n",
      "Epoch 392 of 400\n",
      "Train Loss: 37474.0207\n",
      "Val Loss: 34071.9583\n",
      "Epoch 393 of 400\n",
      "Train Loss: 37474.4770\n",
      "Val Loss: 34116.9277\n",
      "Epoch 394 of 400\n",
      "Train Loss: 37489.6978\n",
      "Val Loss: 34089.9346\n",
      "Epoch 395 of 400\n",
      "Train Loss: 37460.8254\n",
      "Val Loss: 34082.4930\n",
      "Epoch 396 of 400\n",
      "Train Loss: 37463.6384\n",
      "Val Loss: 34104.1064\n",
      "Epoch 397 of 400\n",
      "Train Loss: 37478.1366\n",
      "Val Loss: 34113.7265\n",
      "Epoch 398 of 400\n",
      "Train Loss: 37496.9080\n",
      "Val Loss: 34101.9176\n",
      "Epoch 399 of 400\n",
      "Train Loss: 37534.5517\n",
      "Val Loss: 34091.4840\n",
      "Epoch 400 of 400\n",
      "Train Loss: 37575.1735\n",
      "Val Loss: 34438.1122\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "valid_loss = []\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss = train(\n",
    "        model, train_loader, train_set, device, optimizer, criterion\n",
    "    )\n",
    "    valid_epoch_loss, recon_images = validate(\n",
    "        model, valid_loader, valid_set, device, criterion\n",
    "    )\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    valid_loss.append(valid_epoch_loss)\n",
    "    # save the reconstructed images from the validation loop\n",
    "    save_reconstructed_images(recon_images, epoch+1)\n",
    "    # convert the reconstructed images to PyTorch image grid format\n",
    "    image_grid = make_grid(recon_images.detach().cpu())\n",
    "    grid_images.append(image_grid)\n",
    "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "    print(f\"Val Loss: {valid_epoch_loss:.4f}\")\n",
    "\n",
    "SAVE_DIR = r\"C:\\Users\\trongduong\\[QML Study]\\saved_models\"\n",
    "local_path = \"VAE.pth\"\n",
    "path = os.path.join(SAVE_DIR, local_path)\n",
    "torch.save(model.cpu().state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING COMPLETE\n"
     ]
    }
   ],
   "source": [
    "#save the reconstructions as a .gif file\n",
    "image_to_vid(grid_images)\n",
    "#save the loss plots to disk\n",
    "save_loss_plot(train_loss, valid_loss)\n",
    "print('TRAINING COMPLETE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QML",
   "language": "python",
   "name": "qml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
